<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xmlns:context="http://www.springframework.org/schema/context"
    xsi:schemaLocation="
    http://www.springframework.org/schema/beans
    http://www.springframework.org/schema/beans/spring-beans.xsd
    http://camel.apache.org/schema/spring
    http://camel.apache.org/schema/spring/camel-spring.xsd
    http://www.springframework.org/schema/context
    http://www.springframework.org/schema/context/spring-context-3.0.xsd"
    >
    <!-- AWS Configuration -->
    <context:property-placeholder location="classpath:/aws.properties" />
    <bean name="awsCredentials" class="com.amazonaws.auth.BasicAWSCredentials">
        <constructor-arg value="${access.key}"/>
        <constructor-arg value="${secret.key}"/>
    </bean>
    <bean name="sqsClient" class="com.amazonaws.services.sqs.AmazonSQSAsyncClient">
        <constructor-arg>
            <ref bean="awsCredentials"/>
        </constructor-arg>
    </bean>
    <bean name="snsClient" class="com.amazonaws.services.sns.AmazonSNSClient">
        <constructor-arg>
            <ref bean="awsCredentials"/>
        </constructor-arg>
    </bean>
    <bean name="s3Client" class="com.amazonaws.services.s3.AmazonS3Client">
        <constructor-arg>
            <ref bean="awsCredentials"/>
        </constructor-arg>
    </bean>
    
    <!-- Zip data format -->
    <bean id="zip" class="org.apache.camel.dataformat.zipfile.ZipFileDataFormat">
        <property name="usingIterator" value="false"/>
    </bean>
    <!-- Set a shorter timeout for shutdown, primarily for testing purposes. Default is 300. -->
    <bean id="shutdown" class="org.apache.camel.impl.DefaultShutdownStrategy">
        <property name="timeout" value="30"/>
    </bean>
    <!-- Configure display of trace log messages -->
    <bean id="traceFormatter" class="org.apache.camel.processor.interceptor.DefaultTraceFormatter">
        <property name="showBody" value="false"/>
        <property name="showHeaders" value="false"/>
    </bean>
    <!-- Add an event notifier to log throughput -->
    <!-- <bean id="eventTimer" class="edu.harvard.libcomm.pipeline.EventTimer"/> -->
    <!-- Definition of beans that handle processing of items in the pipeline -->
    <bean id="marcSplitter" class="edu.harvard.libcomm.pipeline.Splitter">
        <property name="splitter">
            <bean class="edu.harvard.libcomm.pipeline.marc.MarcSplitter"/>
        </property>
    </bean>	

	<bean id="deleteSplitter" class="edu.harvard.libcomm.pipeline.Splitter">
        <property name="splitter">
            <bean class="edu.harvard.libcomm.pipeline.delete.DeleteFileSplitter"/>
        </property>
    </bean>	

    <bean id="eadSplitter" class="edu.harvard.libcomm.pipeline.Splitter">
        <property name="splitter">
            <bean class="edu.harvard.libcomm.pipeline.ead.EADSplitter"/>
        </property>
    </bean>
    <bean id="eadRawSplitter" class="edu.harvard.libcomm.pipeline.RawSplitter">
        <property name="splitter">
            <bean class="edu.harvard.libcomm.pipeline.ead.EADRawSplitter"/>
        </property>
    </bean>
    <bean id="viaSplitter" class="edu.harvard.libcomm.pipeline.Splitter">
        <property name="splitter">
            <bean class="edu.harvard.libcomm.pipeline.via.VIASplitter"/>
        </property>
    </bean>
    <bean id="viaRawSplitter" class="edu.harvard.libcomm.pipeline.RawSplitter">
        <property name="splitter">
            <bean class="edu.harvard.libcomm.pipeline.via.VIARawSplitter"/>
        </property>
    </bean>

    <bean id="modsAggregator" class="edu.harvard.libcomm.pipeline.MODSAggregatorStrategy"/>

    <bean id="modsEADRawAggregator" class="edu.harvard.libcomm.pipeline.MODSRawAggregatorStrategy">
        <property name="source" value="OASIS"/>
    </bean>

    <bean id="modsVIARawAggregator" class="edu.harvard.libcomm.pipeline.MODSRawAggregatorStrategy">
        <property name="source" value="VIA"/>
    </bean>

    <bean id="extractPayloadProcessor" class="edu.harvard.libcomm.pipeline.ExtractPayloadProcessor"/>

    <bean id="modsProcessor" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.marc.ModsProcessor"/>
        </property>
    </bean>
    <bean id="filterUnchangedProcessor" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.FilterUnchangedProcessor"/>
        </property>
    </bean>
    <bean id="holdingsProcessor" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.enrich.HoldingsProcessor"/>
        </property>
    </bean>
    <bean id="stackscoreProcessor" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.enrich.StackScoreProcessor"/>
        </property>
    </bean>
    <bean id="lccProcessor" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.enrich.LCCProcessor"/>
        </property>
    </bean>
    <bean id="collectionsProcessor" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.enrich.CollectionsProcessor"/>
        </property>
    </bean>
    <bean id="collectionUpdateProcessor" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.enrich.CollectionUpdateProcessor"/>
        </property>
    </bean>
    <bean id="removeRestrictedProcessor" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.enrich.PublishProcessor"/>
        </property>
    </bean>
    <bean id="solrLoadProcessor" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.solr.SolrProcessor">
                <property name="commitWithinTime" value="30000"/>
            </bean>
        </property>
    </bean>
    <bean id="solrLoadProcessorImmediate" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.solr.SolrProcessor"/>
        </property>
    </bean>
    <bean id="solrDeleteProcessor" class="edu.harvard.libcomm.pipeline.LibCommProcessor">
        <property name="processor">
            <bean class="edu.harvard.libcomm.pipeline.solr.SolrDeleteProcessor"/>
        </property>
    </bean>

    <bean id="prepareURI" class="edu.harvard.libcomm.pipeline.PrepareURIForHTTPEndpoint" />

    <camelContext id="sqsContext" xmlns="http://camel.apache.org/schema/spring" trace="true">
        <!-- Environment-specific properties -->
        <propertyPlaceholder id="librarycloud-properties" location="classpath:/librarycloud.env.properties" />
        <!-- Error handling behavior -->
        <errorHandler id="eh" redeliveryPolicyRef="myPolicy" type="DeadLetterChannel"
            deadLetterUri="direct:dead-letter"/>
        <redeliveryPolicyProfile id="myPolicy" maximumRedeliveries="0"/>
        <route id="dead-letter">
            <from uri="direct:dead-letter"/>
            <to uri="log:edu.harvard.libcomm.deadletterqueue?level=ERROR"/>
            <!-- Post to the AWS dead letter queue, and handle errors (e.g. file size exceeded) -->
            <doTry>
                <transform>
                    <simple>${exception}${exception.stacktrace}${in.body}</simple>
                </transform>
                <to uri="aws-sqs://{{librarycloud.sqs.environment}}-dead-letter?amazonSQSClient=#sqsClient"/>
                <doCatch>
                    <!-- If can't post the full message to the dead letter queue, post only the headers to the queue -->
                    <exception>java.lang.Exception</exception>
                    <to uri="log:edu.harvard.libcomm.deadletterqueue?level=ERROR&amp;showAll=true"/>
                    <transform>
                        <simple>Error placing failed message on dead letter queue. Headers for affected message: ${headers}. Exception: ${exception}${exception.stacktrace}</simple>
                    </transform>
                    <doTry>
                        <to uri="aws-sqs://{{librarycloud.sqs.environment}}-dead-letter?amazonSQSClient=#sqsClient"/>
                        <doCatch>
                            <!-- If can't even post the headers to the dead letter queue, log error and be done -->
                            <exception>java.lang.Exception</exception>
                            <transform>
                                <simple>Error placing failed message headers on dead letter queue. Headers for affected message: ${headers} Exception: ${exception}${exception.stacktrace}</simple>
                            </transform>
                            <to uri="log:edu.harvard.libcomm.deadletterqueue?level=ERROR&amp;showAll=true"/>
                        </doCatch>
                    </doTry>
                </doCatch>
            </doTry>
        </route>

        <!-- ============================== -->
        <!-- Aleph ingest and normalization -->
        <!-- ============================== -->

        <!-- Take a command file, which references a MARC file. Download the MARC file and split it into a 
             series of messages containing MARCXML -->
        <route id="aleph-ingest-split" errorHandlerRef="eh">
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-ingest-aleph?amazonSQSClient=#sqsClient&amp;visibilityTimeout=1800" />
            <!-- Try block required for errors thrown out of the splitter -->
            <doTry>
                <split streaming="true" parallelProcessing="true">
                    <method bean="marcSplitter" method="split"/>
                    <to uri="aws-sqs://{{librarycloud.sqs.environment}}-normalize-marcxml?amazonSQSClient=#sqsClient" />
                </split>
                <doCatch>
                    <exception>java.lang.Exception</exception>
                    <to uri="direct:dead-letter" />
                </doCatch>
            </doTry>
         </route>
         
        <!-- Read messages containing MARCXML from S3 and place them in an in-memory queue for processing -->         
        <route id="marctomods-seda" errorHandlerRef="eh">
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-normalize-marcxml?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
            <to uri="seda:normalize-marcxml?size=20&amp;blockWhenFull=true"/>
        </route>
        
        <!-- Read an in-memory queue to get messages containing MARCXML, and route to two locations. 
             One copy is transformed to MODS and sent through the rest of the pipeline. 
             The other copy is used to upload raw MARCXML data to S3 -->
        <route id="multicast-marc" errorHandlerRef="eh">
            <from uri="seda:normalize-marcxml?concurrentConsumers=4" />
            <multicast parallelProcessing="true">
                <to uri="direct:normalize-marcxml"/>
                <to uri="direct:save-raw-marcxml"/>
            </multicast>            
        </route>
        
        <!-- Transform MARCXML to MODS -->
        <route id="marctomods" errorHandlerRef="eh">
            <from uri="direct:normalize-marcxml" />
            <process ref="modsProcessor"/>
            <to uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-start?amazonSQSClient=#sqsClient" />
        </route>

        <!-- Extract the aleph ID from the MARCXML, and upload the record to S3, keyed by aleph ID-->
        <route id="save-raw-marcxml" errorHandlerRef="eh">
            <from uri="direct:save-raw-marcxml" />
            <process ref="extractPayloadProcessor" />
            <split>
                <tokenize token="record" xml="true" />
                <setHeader headerName="recordFileName">
                    <xpath resultType="java.lang.String">
                        /record/controlfield[@tag='001']
                    </xpath>
                </setHeader>
                <setHeader headerName="CamelAwsS3Key">
                    <simple>${header.recordFileName.substring(0,9)}</simple>
                </setHeader>
                <setHeader headerName="CamelAwsS3CannedAcl">
                    <simple>PublicRead</simple>
                </setHeader>
                <to uri="aws-s3://harvard.librarycloud.original.{{librarycloud.sqs.environment}}?amazonS3Client=#s3Client" />
            </split>
        </route>

        <!-- ============================== -->
        <!-- OASIS ingest and normalization -->
        <!-- ============================== -->
        <route id="oasis-ingest-seda" errorHandlerRef="eh">
            <!-- <from uri="file://{{librarycloud.files.basepath}}/ingest-oasis" /> -->
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-ingest-oasis?amazonSQSClient=#sqsClient&amp;visibilityTimeout=1800&amp;extendMessageVisibility=true" />
            <to uri="seda:oasis-ingest-seda?size=4&amp;blockWhenFull=true"/>
        </route>

        <route id="oasis-ingest" errorHandlerRef="eh">
            <from uri="seda:oasis-ingest-seda?concurrentConsumers=4" />
            <!-- Try block required for errors thrown out of the splitter -->
            <doTry>
                <split streaming="true" parallelProcessing="true">
                    <method bean="eadSplitter" method="split"/>
                    <aggregate strategyRef="modsAggregator" completionSize="100" completionInterval="10000">
                        <correlationExpression>
                            <simple>all</simple>
                        </correlationExpression>
                        <completionPredicate>
                            <simple>${header.messageLength} > 150000</simple>
                        </completionPredicate>  
                        <to uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-start?amazonSQSClient=#sqsClient" />
                        <!-- <to uri="file://{{librarycloud.files.basepath}}/enrich-start?fileName=${header.CamelSplitIndex}" /> -->
                    </aggregate>
                </split>
                <doCatch>
                    <exception>java.lang.Exception</exception>
                    <to uri="direct:dead-letter" />
                </doCatch>
            </doTry>
        </route>

        <!-- VIA ingest and normalization -->
        <route id="via-ingest" errorHandlerRef="eh">
            <!-- <from uri="file://{{librarycloud.files.basepath}}/ingest-via" />             -->
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-ingest-via?amazonSQSClient=#sqsClient&amp;visibilityTimeout=1800&amp;extendMessageVisibility=true" />
            <split>
                <xtokenize mode="t">//filepath</xtokenize>
                <bean ref="prepareURI" method="prepare"/>
                <!-- Download the file identified in the message header. This URI (http://www.example.com) is ignored -->
                <to uri="http://www.example.com" />
                <!-- <to uri="file://{{librarycloud.files.basepath}}/ingest-via-file-url?fileName=via-file-${id}" /> -->
                <to uri="direct:via-ingest-file"/>
            </split>
        </route>
        <route id="via-ingest-file" errorHandlerRef="eh">
            <from uri="direct:via-ingest-file"/>
            <doTry>
                <split streaming="true">
                    <tokenize token="viaRecord" xml="true"/>          
                    <setHeader headerName="outerSplitIndex">
                        <simple>${header.CamelSplitIndex}</simple>
                    </setHeader>
    
                    <split streaming="true" parallelProcessing="true">
                        <method bean="viaRawSplitter" method="split"/>
                        <aggregate strategyRef="modsVIARawAggregator" completionSize="100" completionInterval="10000">
                            <correlationExpression>
                                <simple>all</simple>
                            </correlationExpression>
                            <completionPredicate>
                                <simple>${header.messageLength} > 150000</simple>
                            </completionPredicate>  
                            <to uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-start?amazonSQSClient=#sqsClient" />
                            <!-- <to uri="file://{{librarycloud.files.basepath}}/enrich-start?fileName=${header.outerSplitIndex}-${header.CamelSplitIndex}" /> -->
                        </aggregate>
                    </split>
                </split>
                <doCatch>
                    <exception>java.lang.Exception</exception>
                    <to uri="direct:dead-letter" />
                </doCatch>
            </doTry>
        </route>

        <!-- Route messages to the correct part of the enrich pipeline -->
        <route id="enrich-start" errorHandlerRef="eh">
            <!-- <from uri="file://{{librarycloud.files.basepath}}//enrich-start" /> -->
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-start?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
            <choice>
                <when>
                    <xpath>//source = 'aleph'</xpath>
                    <to uri="seda:enrich-01?size=20&amp;blockWhenFull=true" />
                </when>
                <when>
                    <xpath>//source = 'ALEPH'</xpath>
                    <to uri="seda:enrich-01?size=20&amp;blockWhenFull=true" />
                </when>
                <otherwise>
                    <!-- Skip Holdings, Stackscore, and LCCSH steps for non-aleph records -->
                    <to uri="seda:enrich-04?size=20&amp;blockWhenFull=true" />
                </otherwise>
            </choice>
        </route>

        <route id="addholdingstomods-seda" errorHandlerRef="eh">
            <from uri="direct:enrich-01"/>
            <to uri="seda:enrich-01?size=20&amp;blockWhenFull=true" />
        </route>
        <route id="addholdingstomods" errorHandlerRef="eh">
            <from uri="seda:enrich-01?concurrentConsumers=1"/>
            <process ref="holdingsProcessor"/>
            <to uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-02?amazonSQSClient=#sqsClient" />
            <!-- <to uri="file://{{librarycloud.files.basepath}}/enrich-02" /> -->
        </route>

        <route id="addstackscoretomods-seda" errorHandlerRef="eh">
            <!-- <from uri="file://{{librarycloud.files.basepath}}//enrich-02" /> -->
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-02?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
            <to uri="seda:enrich-02?size=20&amp;blockWhenFull=true" />
        </route>
        <route id="addstackscoretomods" errorHandlerRef="eh">
            <from uri="seda:enrich-02?concurrentConsumers=1"/>
            <process ref="stackscoreProcessor"/>
            <to uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-03?amazonSQSClient=#sqsClient" />
            <!-- <to uri="file://{{librarycloud.files.basepath}}/enrich-03" /> -->
        </route>

        <route id="addslcctomods-seda" errorHandlerRef="eh">
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-03?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
            <to uri="seda:enrich-03?size=20&amp;blockWhenFull=true" />
        </route>
        <route id="addslcctomods" errorHandlerRef="eh">
            <from uri="seda:enrich-03?concurrentConsumers=1"/>
            <process ref="lccProcessor"/>
            <to uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-04?amazonSQSClient=#sqsClient" />
        </route> 

        <route id="addcollectionstomods-seda" errorHandlerRef="eh">
            <!-- <from uri="file://{{librarycloud.files.basepath}}//enrich-04" /> -->
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-04?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
            <to uri="seda:enrich-04?size=20&amp;blockWhenFull=true" />
        </route>
        <route id="addcollectionstomods" errorHandlerRef="eh">
            <from uri="seda:enrich-04?concurrentConsumers=5"/>
            <process ref="collectionsProcessor"/>
            <to uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-end?amazonSQSClient=#sqsClient" />
            <!-- <to uri="file://{{librarycloud.files.basepath}}/enrich-end" /> -->
        </route>

        <!-- Publishing records to consumers -->
        <route id="publish-seda" errorHandlerRef="eh">
            <!-- <from uri="file://{{librarycloud.files.basepath}}/enrich-end" /> -->
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-enrich-end?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
            <to uri="seda:enrich-end?size=20&amp;blockWhenFull=true"/>
        </route>
        <route id="publish" errorHandlerRef="eh">
            <from uri="seda:enrich-end?concurrentConsumers=1" />
            <process ref="removeRestrictedProcessor"/>
            
            <!-- Commenting out unchanged filter until we are set to deploy the associated database -->
            <!-- <process ref="filterUnchangedProcessor"/> -->
            
            <!-- Don't try to publish empty messages -->
            <choice>
                <when>
                    <xpath>string-length(/lib_comm_message/payload/data) = 0</xpath>
                    <to uri="aws-sqs://{{librarycloud.sqs.environment}}-empty-message?amazonSQSClient=#sqsClient" />
                </when>
                <otherwise>
                    <!-- <to uri="file://{{librarycloud.files.basepath}}/publish-public" /> -->
                    <multicast>
                        <to uri="aws-sqs://{{librarycloud.sqs.environment}}-publish-public?amazonSQSClient=#sqsClient" />
                        <to uri="aws-sns://harvard-librarycloud-publish-full-{{librarycloud.sqs.environment}}?amazonSNSClient=#snsClient" />
                        <!-- <to uri="file://{{librarycloud.files.basepath}}/publish-public" /> -->
                        <to uri="direct:compressfile" />
                    </multicast>
                </otherwise>
            </choice>
        </route>
        <route id="compressedFile"  errorHandlerRef="eh">
            <from uri="direct:compressfile"/>
            <marshal ref="zip"/>
            <to uri="file://{{librarycloud.files.basepath}}/publish-public" />
        </route>


        <!-- SOLR consumer for loading into item API -->
        <route id="modstosolr-seda" errorHandlerRef="eh">
            <!--<from uri="file://{{librarycloud.files.basepath}}/publish-public-to" /> -->
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-publish-public?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
            <to uri="seda:publish-public?size=20&amp;blockWhenFull=true"/>
        </route>
        <route id="modstosolr" errorHandlerRef="eh">
            <from uri="seda:publish-public?concurrentConsumers=1" />
            <process ref="solrLoadProcessor"/>
            <to uri="log:edu.harvard.libcomm.throughput?level=TRACE&amp;marker=solrLoadProcessor&amp;groupSize=10"/>
            <to uri="aws-sqs://{{librarycloud.sqs.environment}}-done?amazonSQSClient=#sqsClient"/>
            <!-- <to uri="file://{{librarycloud.files.basepath}}/done" /> -->
        </route>

        <!-- SOLR consumer for loading into item API where the updates should be commited immediately -->
        <route id="modstosolr-immediate" errorHandlerRef="eh">
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-publish-public-immediate?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
            <process ref="solrLoadProcessorImmediate"/>
            <to uri="aws-sqs://{{librarycloud.sqs.environment}}-done?amazonSQSClient=#sqsClient"/>        
        </route>

        <!-- Delete handling -->
         <route id="aleph-delete-ingest-split">
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-delete-aleph?amazonSQSClient=#sqsClient&amp;visibilityTimeout=1800" />
            <!-- Try block required for errors thrown out of the splitter -->
            <doTry>
                <split streaming="true">
                    <method bean="deleteSplitter" method="split"/>
                    <to uri="aws-sqs://{{librarycloud.sqs.environment}}-delete-public?amazonSQSClient=#sqsClient" />
                </split>
                <doCatch>
                    <exception>java.lang.Exception</exception>
                    <to uri="direct:dead-letter" />
                </doCatch>                
            </doTry>
         </route>
        <route id="deletesolr" errorHandlerRef="eh">
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-delete-public?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
            <process ref="solrDeleteProcessor"/>
            <to uri="aws-sqs://{{librarycloud.sqs.environment}}-done?amazonSQSClient=#sqsClient"/>          
        </route>

        <!-- Collection update handling -->
        <route id="updatecollections-seda" errorHandlerRef="eh">
        <!--<from uri="file://{{librarycloud.files.basepath}}/updateCollection-input" /> -->
            <from uri="aws-sqs://{{librarycloud.sqs.environment}}-update-public?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
            <to uri="seda:update-public?size=20&amp;blockWhenFull=true" />
        </route>

        <route id="updateCollection" errorHandlerRef="eh">
            <from uri="seda:update-public?concurrentConsumers=1"/>
            <process ref="collectionUpdateProcessor"/>
            <choice>
                <when>
                    <xpath>string-length(/lib_comm_message/payload/data) = 0</xpath>
                    <to uri="aws-sqs://{{librarycloud.sqs.environment}}-empty-message?amazonSQSClient=#sqsClient" />
                </when>
                <otherwise>
                    <to uri="aws-sqs://{{librarycloud.sqs.environment}}-publish-public-immediate?amazonSQSClient=#sqsClient&amp;maxMessagesPerPoll=10" />
                </otherwise>
            </choice>
        </route>

    </camelContext>
</beans>
